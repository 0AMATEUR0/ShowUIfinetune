{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from PIL import Image, ImageDraw\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import BitsAndBytesConfig, AutoProcessor, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from model.utils import find_target_linear_names\n",
    "from main.trainer import train\n",
    "from main.eval_aitw import validate_aitw\n",
    "from main.eval_mind2web import validate_mind2web\n",
    "from main.eval_screenspot import validate_screenspot\n",
    "from main.evaluator import validate as validate_default\n",
    "from data.dataset import HybridDataset, collate_fn\n",
    "from utils.utils import save_args_to_json, create_log_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    # wandb配置参数\n",
    "    \"wandb_key\": \"7dd9f7e0d1d48f0b0296d469ce4b6365e615094d\", # 修改为你的wandb API key\n",
    "\n",
    "    # 需要修改的路径参数\n",
    "    \"model_path\": \"D:/Project/showui-2b\", # 修改为你的基模型路径\n",
    "    \"train_dataset\": \"screenspot\", # 修改为你的训练数据集路径\n",
    "    \"train_json\": \"metadata\", # 修改为你的训练数据集标注文件名\n",
    "    \"val_dataset\": \"screenspot\", # 修改为你的验证数据集路径\n",
    "    \"val_json\": \"metadata\", # 修改为你的验证数据集标注文件名\n",
    "    \"dataset_dir\": \"D:/Project/my_dataset\", # 修改为你的数据集目录路径\n",
    "    \"exp_dir\": \"D:/Project/MODELS\", # 请修改为你的LoRA权重保存路径\n",
    "\n",
    "    # 模型配置参数\n",
    "    \"model_id\": \"local_ShowUI-2B\", # 模型ID\n",
    "    \"version\": \"showlab/ShowUI-2B\", # 模型版本路径\n",
    "    \"min_visual_tokens\": 256, # 最小视觉token数量\n",
    "    \"max_visual_tokens\": 1280, # 最大视觉token数量\n",
    "    \"model_max_length\": 8192, # 模型最大长度，8192表示支持长文本输入\n",
    "\n",
    "    # ui图配置参数\n",
    "    \"uigraph_train\": True, # Enable ui graph during training\n",
    "    \"uigraph_test\": False, # Enable ui graph during inference\n",
    "    \"uigraph_diff\": 1, # Pixel difference used for constructing ui graph\n",
    "    \"uigraph_rand\": False, # Enable random graph construction\n",
    "    \"uimask_pre\": True, # Prebuild patch selection mask in the preprocessor (not in model layers) for efficiency\n",
    "    \"uimask_ratio\": 0.5, # Specify the percentage of patch tokens to skip per component\n",
    "    \"uimask_rand\": False, # Enable random token selection instead of uniform selection\n",
    "    \n",
    "    \"precision\": \"bf16\", # precision for inference, options: \"fp16\", \"bf16\", \"fp32\"\n",
    "    \"use_qlora\": False, # Use QLoRA for training\n",
    "    \n",
    "    # 语言和视觉层跳过参数\n",
    "    \"lm_skip_ratio\": 0.5, # Ratio of language tokens to skip, e.g., 0.5 means skip 50% of language tokens\n",
    "    \"lm_skip_layer\": '[1,28,0]', # Skip layers for language tokens, e.g., [1,28,0] means skip layer 1 and 28\n",
    "    \"vis_skip_layer\": '[1,32,0]', # Skip layers for visual tokens, e.g., [1,32,0] means skip layer 1 and 32\n",
    "    \"attn_imple\": \"sdpa\", # Attention implementation, options: \"eager\", \"flash_attention_2\", \"sdpa\"\n",
    "    \n",
    "    # LoRA微调配置参数\n",
    "    \"use_qlora\": False, # Whether to use QLoRA for training\n",
    "    \"lora_r\": 8, # Rank for LoRA\n",
    "    \"lora_alpha\": 16, # Alpha for LoRA\n",
    "    \"lora_dropout\": 0.05, # Dropout for LoRA\n",
    "    \"lora_target_modules\": \"qkv_proj\", # Target modules for LoRA, e.g., \"q_proj,v_proj,k_proj,o_proj\"\n",
    "    \"tune_visual_encoder\": False, # Whether to tune the visual encoder\n",
    "    \"freeze_lm_embed\": False, # Whether to freeze the language model embedding\n",
    "\n",
    "    # 梯度检查点配置参数\n",
    "    \"gradient_checkpointing\": True, # Enable gradient checkpointing to reduce memory usage\n",
    "    \"tune_visual_encoder_projector\": False, # Whether to tune the visual encoder projector\n",
    "\n",
    "    # 数据集配置参数\n",
    "    \"train_ratio\": \"1.0\", # Ratio of training data to use, can be a float between 0 and 1\n",
    "    \"val_ratio\": \"1.0\", # Ratio of validation data to use, can be a float between 0 and 1\n",
    "    \"uniform_sample\": False, # Whether to use uniform sampling for training data\n",
    "    \"random_sample\": False, # Whether to use random sampling for training data\n",
    "    \"record_sample\": False, # Whether to record the sampled data for debugging\n",
    "\n",
    "    # 训练配置参数\n",
    "    \"log_base_dir\": \"D:/Project/logs\", # Base directory for logs\n",
    "    \"exp_id\": \"debug\", # Experiment ID for logging\n",
    "    \"lr\": 5e-5, # Learning rate for training\n",
    "    \"beta1\": 0.9, # Beta1 for Adam optimizer\n",
    "    \"beta2\": 0.999, # Beta2 for Adam optimizer\n",
    "    \"epochs\": 10, # Number of epochs for training\n",
    "    \"steps_per_epoch\": 5, # Steps per epoch for training\n",
    "    \"warmup_steps\": 100, # Warmup steps for learning rate scheduler\n",
    "    \"batch_size\": 1, # Batch size for training\n",
    "    \"grad_accumulation_steps\": 8, # Gradient accumulation steps\n",
    "    \"val_batch_size\": 1, # Batch size for validation\n",
    "    \"workers\": 4, # Number of workers for data loading\n",
    "\n",
    "    # Grounding setting\n",
    "    \"num_turn\": 100, # Interleaved Query-Action setting\n",
    "    \"shuffle_image_token\": False, # shuffle image token for training\n",
    "    \"uniform_prompt\": True, # Use uniform prompt for training\n",
    "    \"text2point\": 1.0, # Text to point ratio for training\n",
    "    \"text2bbox\": 0.0, # Text to bbox ratio for training\n",
    "    \"point2text\": 0.0, # Point to text ratio for training\n",
    "    \"bbox2text\": 0.0, # Bbox to text ratio for training\n",
    "    \"crop_min\": 1.0 , # Minimum crop ratio for training\n",
    "    \"crop_max\": 1.0, # Maximum crop ratio for training\n",
    "    \"xy_int\": False, # Whether to use integer coordinates for x and y in grounding\n",
    "\n",
    "    # Navigation setting\n",
    "    \"num_history\": 4, # Number of history steps for navigation\n",
    "    \"interleaved_history\": 'tttt', # Interleaved Vision-Action setting,choices=['tttt', 'vvvv', 'vtvt', 'tvtv', 'vvtt', 'ttvv']\n",
    "    \"skip_readme_train\": False, # Whether to skip README training data\n",
    "    \"skip_readme_test\": False, # Whether to skip README test data\n",
    "\n",
    "    # 模型检测点和评估配置参数\n",
    "    \"eval_only\": False, # Whether to only run evaluation\n",
    "    \"start_epoch\": 0, # Start epoch for training\n",
    "    \"no_eval\": False, # Whether to skip evaluation during training\n",
    "    \"debug\": False, # for debugging, will not save model and monitor\n",
    "    \"print_freq\": 1, # Frequency of printing training progress\n",
    "\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "# 如果args是字典，将其转换为SimpleNamespace对象\n",
    "if isinstance(args, dict):\n",
    "    args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18458e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型主函数\n",
    "def ShowUItrain(args):\n",
    "\n",
    "    args.global_rank = int(os.environ.get(\"RANK\", 0))\n",
    "    args.local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n",
    "    args.world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "\n",
    "    if args.attn_imple in [\"eager\", \"sdpa\"]:\n",
    "        # suggested by https://github.com/Lightning-AI/litgpt/issues/327\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "        torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "    args.distributed = args.world_size > 1\n",
    "\n",
    "\n",
    "    args.log_dir = os.path.join(args.log_base_dir, args.exp_id, timestamp)\n",
    "    args.tmp_dir = os.path.join(args.log_dir, \"tmp\")\n",
    "\n",
    "    # must provide wandb-key\n",
    "    assert args.wandb_key is not None\n",
    "    wandb.login(key=args.wandb_key)\n",
    "\n",
    "\n",
    "    writer = None  # TensorBoard writer, if needed, can be initialized later\n",
    "    os.makedirs(args.log_dir, exist_ok=True)\n",
    "    os.makedirs(args.tmp_dir, exist_ok=True)\n",
    "    save_args_to_json(args, os.path.join(args.log_dir, \"args.json\"))  # 保存参数\n",
    "    if not args.debug:\n",
    "        # 创建TensorBoard日志目录\n",
    "        writer = SummaryWriter(os.path.join(args.log_dir, \"tensorboard\"))\n",
    "        # 初始化wandb\n",
    "        wandb.init(\n",
    "            project=\"ShowUI\",\n",
    "            group=args.exp_id,\n",
    "            name=f'{args.exp_id}_{timestamp}',\n",
    "            config=args,\n",
    "            dir=args.log_dir,\n",
    "        )\n",
    "    print(f\"Start Job: {args.exp_id}\")\n",
    "\n",
    "    # 创建处理器\n",
    "\n",
    "    from model.showui.processing_showui import ShowUIProcessor\n",
    "\n",
    "    processor = ShowUIProcessor.from_pretrained(args.model_path,\n",
    "                                                min_pixels=args.min_visual_tokens *28*28,\n",
    "                                                max_pixels=args.max_visual_tokens *28*28,\n",
    "                                                model_max_length=args.model_max_length,\n",
    "                                                uigraph_train=args.uigraph_train, uigraph_test=args.uigraph_test,\n",
    "                                                uigraph_diff=args.uigraph_diff,  uigraph_rand=args.uigraph_rand,\n",
    "                                                uimask_pre=args.uimask_pre, uimask_ratio=args.uimask_ratio, uimask_rand=args.uimask_rand,\n",
    "                                                size = {\"shortest_edge\": 3136, \"longest_edge\": 1003520}\n",
    "                                              )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # 创建模型\n",
    "    torch_dtype = torch.float32\n",
    "    if args.precision == \"bf16\":\n",
    "        torch_dtype = torch.bfloat16\n",
    "    elif args.precision == \"fp16\":\n",
    "        torch_dtype = torch.half\n",
    "\n",
    "    model_path = args.model_path\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    llm_int8_skip_modules=[\"img_projection\"],\n",
    "                ) if args.use_qlora else None # 仅在使用QLoRA时才需要配置\n",
    "    \n",
    "    from model.utils import parse_layer_type\n",
    "    from model.showui.modeling_showui import ShowUIForConditionalGeneration\n",
    "\n",
    "    lm_qwen_layer = 28\n",
    "    vis_qwen_layer = 32\n",
    "    lm_skip_layer = parse_layer_type(args.lm_skip_layer, lm_qwen_layer)\n",
    "    vis_skip_layer = parse_layer_type(args.vis_skip_layer, vis_qwen_layer)\n",
    "\n",
    "    model = ShowUIForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch_dtype, # 模型精度\n",
    "        low_cpu_mem_usage=True, # 低内存使用模式\n",
    "        _attn_implementation=args.attn_imple, # 注意力实现方式\n",
    "        # quantization_config=bnb_config, # 量化配置\n",
    "        device_map=\"cuda\", # 自动设备映射\n",
    "        lm_skip_layer=lm_skip_layer, # 跳过语言层\n",
    "        lm_skip_ratio=args.lm_skip_ratio, # 跳过语言层比例\n",
    "        tie_word_embeddings=False, # 是否共享词嵌入\n",
    "    )\n",
    "\n",
    "    # 保存 untied 模型\n",
    "    model.save_pretrained(\"D:/Project/MODELS\")\n",
    "    model.config.save_pretrained(\"D:/Project/MODELS\")\n",
    "\n",
    "\n",
    "    # 加载模型检测点\n",
    "    # if args.version != args.model_id:\n",
    "    #     state_dict = torch.load(args.version, map_location=\"cpu\")\n",
    "    #     model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    model.config.use_cache = False # 禁用缓存以节省内存\n",
    "\n",
    "    # 在评估模式下，不需要加载LoRA\n",
    "    if args.eval_only:\n",
    "        print(\"evaluation mode, thus set the `lora_r' as zero.\")\n",
    "        args.lora_r = 0\n",
    "    if not args.eval_only and args.use_qlora:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # 配置LoRA\n",
    "    lora_r = args.lora_r\n",
    "    if lora_r > 0:\n",
    "        lora_alpha = args.lora_alpha\n",
    "        lora_dropout = args.lora_dropout\n",
    "        exclude_module = [\"visual\"] if not args.tune_visual_encoder else []\n",
    "        exclude_module += [\"lm_head\"] if args.freeze_lm_embed else exclude_module\n",
    "        lora_target_modules = find_target_linear_names(model, lora_namespan_exclude=exclude_module)\n",
    "\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=lora_target_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        # model.print_trainable_parameters()\n",
    "\n",
    "        # 如果使用LoRA，则原始模型被包装2次\n",
    "        # 一次是peft的get_peft_model包装，一次是ShowUIForConditionalGeneration的包装\n",
    "        model_child = model.model.model # 获取原始模型，疑似不可使用base_model方法\n",
    "    else:\n",
    "        # 如果不使用LoRA，则原始模型只被ShowUIForConditionalGeneration包装\n",
    "        model_child = model.model\n",
    "    \n",
    "    # 梯度检查点，降低显存使用\n",
    "    if args.gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.enable_input_require_grads()\n",
    "    \n",
    "    if not args.tune_visual_encoder:\n",
    "        # 冻结视觉编码器\n",
    "        if args.lora_r > 0:\n",
    "            for p in model.base_model.model.visual.parameters():\n",
    "                p.requires_grad = False\n",
    "        elif args.lora_r == 0:\n",
    "            for p in model.visual.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "    if args.tune_visual_encoder_projector:\n",
    "        for k, p in model.named_parameters():\n",
    "            if 'visual.merger' in k:\n",
    "                p.requires_grad = True\n",
    "    \n",
    "    if args.freeze_lm_embed:\n",
    "        if args.lora_r > 0:\n",
    "            for p in model_child.embed_tokens.parameters():\n",
    "                p.requires_grad = False\n",
    "        elif args.lora_r == 0:\n",
    "            for p in model_child.embed_tokens.parameters():\n",
    "                p.requires_grad = False\n",
    "    \n",
    "    # 检查可训练参数\n",
    "    list_of_params_to_optimize = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            # print(\"[Name]\", n, \" [Shape]\", p.shape)\n",
    "            list_of_params_to_optimize.append(p)\n",
    "    \n",
    "    # 创建数据集\n",
    "    args.samples_per_epoch = args.batch_size    \\\n",
    "                    * args.grad_accumulation_steps  \\\n",
    "                    * args.steps_per_epoch\n",
    "\n",
    "    train_dataset = HybridDataset(\n",
    "        processor,\n",
    "        inference=False,  # 仅用于训练\n",
    "        args=args,\n",
    "    )\n",
    "    \n",
    "    val_dataset = HybridDataset(\n",
    "        processor,\n",
    "        inference=True,  # 仅用于验证\n",
    "        args=args,\n",
    "    )\n",
    "\n",
    "    if args.val_dataset == \"mind2web\":\n",
    "        validate = validate_mind2web\n",
    "    elif args.val_dataset == \"screenspot\":\n",
    "        validate = validate_screenspot\n",
    "    elif args.val_dataset == \"aitw\":\n",
    "        validate = validate_aitw\n",
    "    else:\n",
    "        validate = validate_default\n",
    "\n",
    "    if not args.random_sample:\n",
    "        args.steps_per_epoch = len(train_dataset) // (args.batch_size * args.world_size)\n",
    "\n",
    "    # deepspeed参数（待完成）\n",
    "    # 如果使用DeepSpeed，参考https://github.com/showlab/ShowUI/blob/main/train.py\n",
    "\n",
    "    # LoRA微调\n",
    "    if lora_r > 0:\n",
    "\n",
    "        # 创建优化器\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list_of_params_to_optimize,\n",
    "            lr=args.lr,\n",
    "            betas=(args.beta1, args.beta2),\n",
    "            weight_decay=0.0,\n",
    "            )\n",
    "\n",
    "        # DeepSpeed 用的是 WarmupDecayLR，PyTorch 没有内置这个，但可以用类似的调度器\n",
    "        total_steps = args.epochs * args.steps_per_epoch\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=partial(collate_fn, processor=processor),\n",
    "            num_workers=args.workers,  # 根据你的CPU核心数调整\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 模型引擎\n",
    "        model_engine = model\n",
    "        model_engine = model_engine.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        \n",
    "\n",
    "    # 如果不使用LoRA微调\n",
    "    # 暂时一样，但是方便后续扩展\n",
    "    elif lora_r == 0 and not args.eval_only:\n",
    "        # 创建优化器\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list_of_params_to_optimize,\n",
    "            lr=args.lr,\n",
    "            betas=(args.beta1, args.beta2),\n",
    "            weight_decay=0.0,\n",
    "            )\n",
    "\n",
    "        # DeepSpeed 用的是 WarmupDecayLR，PyTorch 没有内置这个，但可以用类似的调度器\n",
    "        total_steps = args.epochs * args.steps_per_epoch\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=args.warmup_steps,\n",
    "            num_training_steps=total_steps,\n",
    "        )\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=partial(collate_fn, processor=processor),\n",
    "            num_workers=args.workers,  # 根据你的CPU核心数调整\n",
    "        )\n",
    "\n",
    "        # 模型引擎\n",
    "        model_engine = model\n",
    "        model_engine = model_engine.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # 仅评估模式\n",
    "    elif args.eval_only:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False \n",
    "        model_engine = model\n",
    "    else:\n",
    "        raise ValueError(\"Invalid setting\")\n",
    "    \n",
    "\n",
    "    # 断点加载（待完成）\n",
    "\n",
    "    # 验证集\n",
    "    if val_dataset is not None:\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=args.val_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=args.workers,\n",
    "            pin_memory=False,\n",
    "            sampler= None,  # 若分布式训练，此处参考https://github.com/showlab/ShowUI/blob/main/train.py\n",
    "            collate_fn=partial(collate_fn, processor=processor)\n",
    "        )\n",
    "    else:\n",
    "        val_loader = None\n",
    "    \n",
    "    if args.eval_only:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model_engine = model_engine.to(device)\n",
    "        validate(val_loader, model_engine, processor, 0, 0, writer, args)\n",
    "        exit()\n",
    "\n",
    "    train_iter = iter(train_loader)\n",
    "    best_score = 0.0\n",
    "    # args.start_epoch 是为了支持断点恢复训练\n",
    "    print(\"Start training\")\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        # train for one epoch\n",
    "        train_iter, global_step = train(\n",
    "            train_loader,\n",
    "            model_engine,\n",
    "            optimizer,\n",
    "            epoch,\n",
    "            scheduler,\n",
    "            writer,\n",
    "            train_iter,\n",
    "            args,\n",
    "        )\n",
    "\n",
    "        if args.no_eval == False and val_loader is not None:\n",
    "            score = validate(\n",
    "                val_loader,\n",
    "                model_engine,\n",
    "                processor,\n",
    "                epoch,\n",
    "                global_step,\n",
    "                writer,\n",
    "                args,\n",
    "            )\n",
    "            is_best = score > best_score\n",
    "            best_score = max(score, best_score)\n",
    "        else:\n",
    "            is_best = True\n",
    "            score = 0.0\n",
    "        \n",
    "        if args.no_eval or is_best:\n",
    "            save_dir = os.path.join(args.log_dir,\"ckpt_model\")\n",
    "            \n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            torch.save(\n",
    "                {\"epoch\": epoch},\n",
    "                os.path.join(\n",
    "                    save_dir,\n",
    "                    \"meta_log_epo{:.0f}_score{:.2f}.pth\".format(\n",
    "                            epoch, best_score\n",
    "                        ),\n",
    "                ),\n",
    "            )\n",
    "            # if args.distributed:\n",
    "            #     # 确保所有进程都完成保存\n",
    "            #     torch.distributed.barrier()\n",
    "            try:\n",
    "                torch.save(\n",
    "                    model_engine.state_dict(),\n",
    "                    os.path.join(\n",
    "                        save_dir,\n",
    "                        \"model_epo{:.0f}_score{:.2f}.pth\".format(\n",
    "                            epoch, best_score\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(\"Failed to save checkpoint (): \", e)\n",
    "    \n",
    "    \n",
    "    if args.global_rank == 0:\n",
    "        if not args.debug:\n",
    "            wandb.finish()\n",
    "            writer.close()\n",
    "\n",
    "# 合并LoRA权重和原始模型\n",
    "def ShowUImerge(args):\n",
    "    json_url = os.path.join(args.exp_dir, \"args.json\")\n",
    "    with open(json_url,'r') as f:\n",
    "        json_args = json.load(f)\n",
    "    for key, value in json_args.items():\n",
    "        setattr(args, key, value)\n",
    "    \n",
    "    args.save_path = args.exp_dir +\"/ckpt_model/merged_model\"\n",
    "    args.weight_url = args.exp_dir +\"/ckpt_model/adapter_model.safetensors\"\n",
    "\n",
    "    torch_dtype = torch.float32\n",
    "    if args.precision == \"bf16\":\n",
    "        torch_dtype = torch.bfloat16\n",
    "    elif args.precision == \"fp16\":\n",
    "        torch_dtype = torch.half\n",
    "    \n",
    "    from model.showui.processing_showui import ShowUIProcessor\n",
    "\n",
    "    model_url = args.model_path\n",
    "\n",
    "    processor = ShowUIProcessor.from_pretrained(args.model_path,\n",
    "                                                min_pixels=args.min_visual_tokens *28*28,\n",
    "                                                max_pixels=args.max_visual_tokens *28*28,\n",
    "                                                model_max_length=args.model_max_length,\n",
    "                                                uigraph_train=args.uigraph_train, uigraph_test=args.uigraph_test,\n",
    "                                                uigraph_diff=args.uigraph_diff,  uigraph_rand=args.uigraph_rand,\n",
    "                                                uimask_pre=args.uimask_pre, uimask_ratio=args.uimask_ratio, uimask_rand=args.uimask_rand,\n",
    "                                                size = {\"shortest_edge\": 3136, \"longest_edge\": 1003520}\n",
    "                                              )\n",
    "    \n",
    "    from model.utils import parse_layer_type\n",
    "    from model.showui.modeling_showui import ShowUIForConditionalGeneration\n",
    "\n",
    "    lm_qwen_layer = 28\n",
    "    vis_qwen_layer = 32\n",
    "    lm_skip_layer = parse_layer_type(args.lm_skip_layer, lm_qwen_layer)\n",
    "    vis_skip_layer = parse_layer_type(args.vis_skip_layer, vis_qwen_layer)\n",
    "\n",
    "    model = ShowUIForConditionalGeneration.from_pretrained(\n",
    "        args.model_path,\n",
    "        torch_dtype=torch_dtype, # 模型精度\n",
    "        low_cpu_mem_usage=True, # 低内存使用模式\n",
    "        _attn_implementation=args.attn_imple, # 注意力实现方式\n",
    "        # quantization_config=bnb_config, # 量化配置\n",
    "        device_map=\"cuda\", # 自动设备映射\n",
    "        lm_skip_layer=lm_skip_layer, # 跳过语言层\n",
    "        lm_skip_ratio=args.lm_skip_ratio, # 跳过语言层比例\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = False\n",
    "    model.config.tokenizer_model_max_length = processor.tokenizer.model_max_length\n",
    "\n",
    "    lora_r = args.lora_r\n",
    "    if lora_r > 0:\n",
    "        lora_alpha = args.lora_alpha\n",
    "        lora_dropout = args.lora_dropout\n",
    "        lora_target_modules = find_target_linear_names(model, lora_namespan_exclude=[\"visual\"])\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            target_modules=lora_target_modules,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    \n",
    "\n",
    "        # 合并LoRA权重\n",
    "        print(\"Merging LoRA weights...\")\n",
    "        model = model.merge_and_unload()\n",
    "\n",
    "        # 将合并后的模型权重保存到指定路径\n",
    "        print(f\"Saving merged model to {args.save_path}\")\n",
    "        model.save_pretrained(\n",
    "            args.save_path,\n",
    "            max_shard_size=\"10GB\",  # 分片大小\n",
    "            safe_serialization=True,  # 安全序列化\n",
    "        )\n",
    "        processor.save_pretrained(args.save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503774e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练ShowUI模型\n",
    "ShowUItrain(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47da84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并LoRA权重和原始模型\n",
    "ShowUImerge(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4918e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ShowUI模型推理\n",
    "\n",
    "class ShowUI:\n",
    "    def __init__(self, model_path: str, args):\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.args = args\n",
    "\n",
    "    def load_model(self):\n",
    "\n",
    "        torch_dtype = torch.float32\n",
    "        if self.args.precision == \"bf16\":\n",
    "            torch_dtype = torch.bfloat16\n",
    "        elif self.args.precision == \"fp16\":\n",
    "            torch_dtype = torch.half\n",
    "        \n",
    "        print(\"Loading processor...\")\n",
    "        \n",
    "        from model.showui.processing_showui import ShowUIProcessor\n",
    "\n",
    "        self.processor = ShowUIProcessor.from_pretrained(self.args.model_path,\n",
    "                                                min_pixels=self.args.min_visual_tokens *28*28,\n",
    "                                                max_pixels=self.args.max_visual_tokens *28*28,\n",
    "                                                model_max_length=self.args.model_max_length,\n",
    "                                                uigraph_train=self.args.uigraph_train, uigraph_test=self.args.uigraph_test,\n",
    "                                                uigraph_diff=self.args.uigraph_diff,  uigraph_rand=self.args.uigraph_rand,\n",
    "                                                uimask_pre=self.args.uimask_pre, uimask_ratio=self.args.uimask_ratio, uimask_rand=self.args.uimask_rand,\n",
    "                                                size = {\"shortest_edge\": 3136, \"longest_edge\": 1003520}\n",
    "                                              )\n",
    "        \n",
    "        print(\"Processor loaded successfully.\")\n",
    "\n",
    "        print(f\"Loading model from {self.model_path}...\")\n",
    "\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    llm_int8_skip_modules=[\"img_projection\"],\n",
    "                ) if self.args.use_qlora else None\n",
    "\n",
    "        from model.utils import parse_layer_type\n",
    "        from model.showui.modeling_showui import ShowUIForConditionalGeneration\n",
    "\n",
    "        lm_qwen_layer = 28\n",
    "        vis_qwen_layer = 32\n",
    "        lm_skip_layer = parse_layer_type(self.args.lm_skip_layer, lm_qwen_layer)\n",
    "        vis_skip_layer = parse_layer_type(self.args.vis_skip_layer, vis_qwen_layer)\n",
    "\n",
    "        self.model = ShowUIForConditionalGeneration.from_pretrained(\n",
    "                                                    self.args.model_path,\n",
    "                                                    torch_dtype=torch_dtype, # 模型精度\n",
    "                                                    low_cpu_mem_usage=True, # 低内存使用模式\n",
    "                                                    _attn_implementation=self.args.attn_imple, # 注意力实现方式\n",
    "                                                    # quantization_config=bnb_config, # 量化配置\n",
    "                                                    device_map=\"cuda\", # 自动设备映射\n",
    "                                                    lm_skip_layer=lm_skip_layer, # 跳过语言层\n",
    "                                                    lm_skip_ratio=self.args.lm_skip_ratio, # 跳过语言层比例\n",
    "    )\n",
    "        \n",
    "        print(\"Model loaded successfully.\")\n",
    "\n",
    "    def invoke(self, img_url: str, query: str, args):\n",
    "        image = Image.open(img_url)\n",
    "\n",
    "        print(f\"Image loaded from {img_url}, size: {image.size}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "\n",
    "        print(\"Processing messages for model input...\")\n",
    "\n",
    "        _SYSTEM = (\n",
    "            \"Based on the screenshot of the page, I give a text description and you give its corresponding location. \"\n",
    "            \"The coordinate represents a clickable location [x, y] for an element, which is a relative coordinate on the screenshot, scaled from 0 to 1.\"\n",
    "        )\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": _SYSTEM},\n",
    "                    {\"type\": \"image\", \"image\": img_url, \"min_pixels\": self.args.min_visual_tokens * 28 * 28, \"max_pixels\": self.args.max_visual_tokens * 28 * 28},\n",
    "                    {\"type\": \"text\", \"text\": query}\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True,\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "        print(\"Inputs prepared for model generation.\")\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        print(\"Model generation completed.\")\n",
    "        print(\"Decoding generated IDs to text...\")\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        print(f\"Output text: {output_text}\")\n",
    "        print(\"Decoding completed.\")\n",
    "        \n",
    "        click_xy = ast.literal_eval(output_text)\n",
    "        x, y = click_xy[0] * image.width, click_xy[1] * image.height\n",
    "\n",
    "        \n",
    "        return x, y, image\n",
    "\n",
    "    def draw_point(self, image, x, y, radius=2):\n",
    "        print(f\"Drawing point at ({x}, {y}) with radius {radius} on the image.\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.ellipse((x - radius, y - radius, x + radius, y + radius), fill='red', outline='red')\n",
    "        image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 ShowUI 实例并加载模型\n",
    "model_path = \"D:/Project/logs/debug/2025-06-19_21-45-54/ckpt_model/merged_model\"\n",
    "showui = ShowUI(model_path,args)\n",
    "showui.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"D:/Project/my_dataset/unlabel_images/image0.png\"\n",
    "query = \"\"\n",
    "\n",
    "x, y, image = showui.invoke(img_url, query, args)\n",
    "print(f\"Click coordinates: ({x}, {y})\")\n",
    "showui.draw_point(image, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
